<!DOCTYPE html>
<html lang=en>
           <head>
            <meta charset=UTF-8>
            <title>KnowledgeWorks: limits on size of rule base?</title>
            <meta name=viewport
                  content="width=device-width, initial-scale=1.0">
            <link rel=stylesheet
                  href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css
                  integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2
                  crossorigin=anonymous>
            <style>
section.tree {
    padding-left: 2em;
}
section.tree:first-child {
    padding-left: 0;
}
.article-link {
  margin-bottom: 1em;
}
</style>
<!-- Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9X3G9MMWZP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9X3G9MMWZP');
</script>

           </head>
           <body>
            <header class="d-flex justify-content-center">
             <nav
                  class="navbar navbar-light bg-light w-100 mx-5 mb-3">
              <a class=navbar-brand href="/">Lisp HUG Maillist Archive</a>
             </nav>
            </header>
            <div class="d-flex justify-content-center">
             <div class="w-100 mx-5 px-3">
              <section class=tree>
               <article class=email>
                <h1>KnowledgeWorks: limits on size of rule base?</h1>
                <pre>Greetings. We're building an expert system that is composed in large part of
classification-type "rules". These "rules" (quoted here because the
implementation is in question) are generally simple, and lend themselves to
auto-generation. However, in our production system there will realistically
be hundreds of thousands, if not more than a million, such "rules".

We must choose between two implementations. One approach is to store the
classification "rule" data in hash tables that reside in a saved image;
other rules using this data would employ predicates as part of pattern
matching. The other approach is to auto-generate the classification rules
from our database.

I'm looking for advice. Are there limitations within the KW implementation
that would make the table approach preferable to the rule approach, or
vice-versa? If not, from an engineering perspective is one implementation
preferable?

Thanks very much.

David E. Young
Bloodhound Software, Inc.
http://bloodhoundinc.com

"For wisdom is more precious than rubies,
and nothing you desire can compare with her."
  -- Prov. 8:11

"But all the world understands my language."
  -- Franz Joseph Haydn (1732-1809)


________________________________________________________________________
This email has been scanned for all viruses by the MessageLabs Email
Security System. For more information on a proactive email security
service working around the clock, around the globe, visit
http://www.messagelabs.com
________________________________________________________________________

</pre>
               </article>
               <section class=tree>
                <article class=email>
                 <h1>Re: KnowledgeWorks: limits on size of rule base?</h1>
                 <pre>&gt&gt&gt&gt&gt On Wed, 30 Jul 2003 10:21:02 -0400, "Young, David" &lt;dyoung@bloodhoundinc.com&gt said:

    David&gt Greetings. We're building an expert system that is composed in large part of
    David&gt classification-type "rules". These "rules" (quoted here because the
    David&gt implementation is in question) are generally simple, and lend themselves to
    David&gt auto-generation. However, in our production system there will realistically
    David&gt be hundreds of thousands, if not more than a million, such "rules".

    David&gt We must choose between two implementations. One approach is to store the
    David&gt classification "rule" data in hash tables that reside in a saved image;
    David&gt other rules using this data would employ predicates as part of pattern
    David&gt matching. The other approach is to auto-generate the classification rules
    David&gt from our database.

    David&gt I'm looking for advice. Are there limitations within the KW implementation
    David&gt that would make the table approach preferable to the rule approach, or
    David&gt vice-versa? If not, from an engineering perspective is one implementation
    David&gt preferable?

Hi,

I presume that you are talking about forward chaining rules? KW
compiles the left-hand sides of these rules into a network of nodes
(called a RETE network). As each KW object is created or modified it
immediately creates objects that represent partial matches to the
rules. These matches are combined with other matches and flow through
the network. When the inference engine is run the eligible rules are
therefore already known and fire immediately. Each time the right-hand
side of a rule modifies the state another wave of incremental change
propagates through the network and so the eligible rule-set is
efficiently known at each cycle.

So KW goes to some trouble to make matching efficient. However, I
don't really know whether you will find creating rules or testing
properties more efficient. It depends on the complexity of the tests,
the number of objects, the amount of change, etc.

__Jason


</pre>
                </article>
               </section>
              </section>
             </div>
            </div>
            <footer class="d-flex justify-content-center">
             <div>
              Updated at: 2020-12-10 08:59 UTC
             </div>
            </footer>
           </body>
          </html>